{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the data of first 3 movies\n",
    "\n",
    "From this link,\n",
    "Find and print the name and genre of the first 3 titles\n",
    "Output Format :\n",
    "title_name_1 ; genre1_1, genre1_2 ..\n",
    "title_name_2 ; genre2_1, genre2_2 ..\n",
    "title_name_2 ; genre3_1, genre3_2 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers: Infinity War ; Action, Adventure, Sci-Fi\n",
      "Pantera Negra ; Action, Adventure, Sci-Fi\n",
      "Deadpool 2 ; Action, Adventure, Comedy\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "url = 'https://www.imdb.com/search/title?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt'\n",
    "response = requests.get(url)\n",
    "data = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "container = data.find('div',{'class':'lister-list'})\n",
    "movies = data.find_all('div',{'class':'lister-item-content'})\n",
    "\n",
    "for i in movies[:3]:\n",
    "    title = i.a.text\n",
    "    genre = i.find('span',{'class':'genre'}).text\n",
    "    genre = genre.strip()\n",
    "    print(title, ';', genre)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### titles with most votes\n",
    "\n",
    "Link to use\n",
    "Print the names of movies with highest number of votes from year 2010 to 2014\n",
    "Note : Print the titles line wise starting from year 2010\n",
    "Output Format :\n",
    "title_name_1\n",
    "title_name_2\n",
    "title_name_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Year, Votes]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "url = 'https://www.imdb.com/search/title?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt'\n",
    "response = requests.get(url)\n",
    "data = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "columns = ['Title','Year','Votes']\n",
    "df = pd.DataFrame()\n",
    "\n",
    "container = data.find('div',{'class':'lister-list'})\n",
    "movies = data.find_all('div',{'class':'lister-item-content'})\n",
    "\n",
    "for i in movies:\n",
    "    votes = i.find('p',{'class':'sort-num_votes-visible'})\n",
    "    votes = votes.find('span',{'name':'nv'})['data-value']\n",
    "    title = i.a.text\n",
    "    year = i.find('span',{'class':'lister-item-year text-muted unbold'})\n",
    "    year = re.findall(r'\\d+', year.text)[0]\n",
    "    \n",
    "    obj_dict = {'Title':title, 'Year':year, 'Votes':votes}\n",
    "    df_temp = pd.DataFrame([obj_dict])\n",
    "    df = pd.concat([df, df_temp], ignore_index=True)\n",
    "    \n",
    "    \n",
    "df = df.sort_values(by='Votes', ascending=False)\n",
    "top = df[(df['Year']>'2010') & (df['Year']<'2014')]\n",
    "\n",
    "for key, value in top.iterrows():\n",
    "    print(value) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein Tisch ist ein Tisch\n",
      "Liz\n",
      "Terrapin\n",
      "Honesto\n",
      "Sisu\n",
      "Ein Tisch ist ein Tisch\n",
      "Liz\n",
      "Terrapin\n",
      "Honesto\n",
      "Sisu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "names = []\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "for i in range(2010,2015):\n",
    "    page = requests.get(\"http://www.imdb.com/search/title?release_date=\"+str(i)+\"&sort=num_votes, des&page=1&ref_=adv_nxt&Accept_language=en-US,en\")\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    head = soup.find(\"h3\",class_=\"lister-item-header\")\n",
    "    title = head.a.string\n",
    "    print(title)\n",
    "    names.append(title)\n",
    "for i in names :\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title with maximum duration\n",
    "\n",
    "Link to use\n",
    "Out of the first 250 titles with highest number of votes in 2018,find which title has the maximum duration.\n",
    "Output Format :\n",
    "title_name title_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avengers: Infinity War</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pantera Negra</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deadpool 2</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spider-Man: Un nuevo universo</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bohemian Rhapsody, la historia de Freddie Mercury</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>The Resident</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Zero</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Jefa por accidente</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>No te preocupes, no irá lejos</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Ölümlü Dünya</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title Duration\n",
       "0                               Avengers: Infinity War      149\n",
       "1                                        Pantera Negra      134\n",
       "2                                           Deadpool 2      119\n",
       "3                        Spider-Man: Un nuevo universo      117\n",
       "4    Bohemian Rhapsody, la historia de Freddie Mercury      134\n",
       "..                                                 ...      ...\n",
       "243                                       The Resident      164\n",
       "244                                               Zero      103\n",
       "245                                 Jefa por accidente      114\n",
       "246                      No te preocupes, no irá lejos      107\n",
       "247                                       Ölümlü Dünya      103\n",
       "\n",
       "[248 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "columns = ['Title', 'Duration']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "for i in range(0,5):\n",
    "    page = requests.get(\"https://www.imdb.com/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=\"+str((i*50)+1)+\"&ref_=adv_nxt\")\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    head_title = soup.find_all(\"h3\",class_=\"lister-item-header\")\n",
    "    head_duration = soup.find_all(\"span\",class_=\"runtime\")\n",
    "    for i, j in zip(head_title, head_duration):\n",
    "        title = i.a.string\n",
    "        duration = ''.join(filter(str.isdigit, j.string))\n",
    "        # print (str(title), int(duration))\n",
    "        \n",
    "        obj_dict = {'Title':str(title), 'Duration':int(duration)}\n",
    "        df_temp = pd.DataFrame([obj_dict])\n",
    "        df = pd.concat([df, df_temp], ignore_index=True)\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La maldición de Hill House'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=['Duration'], ascending=False)\n",
    "title = df_sort.iloc[0]['Title'].strip(\"'\")\n",
    "title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La maldición de Hill House 572\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "longest_movie = ''\n",
    "longest_duration = 0\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "for i in range(0,5):\n",
    "    page = requests.get(\"https://www.imdb.com/search/title/?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&start=\"+str((i*50)+1)+\"&ref_=adv_nxt\")\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    head_title = soup.find_all(\"h3\",class_=\"lister-item-header\")\n",
    "    head_duration = soup.find_all(\"span\",class_=\"runtime\")\n",
    "    for i, j in zip(head_title, head_duration):\n",
    "        title = i.a.string\n",
    "        duration = ''.join(filter(str.isdigit, j.string))\n",
    "        duration = int(duration)\n",
    "        \n",
    "        if (duration > longest_duration):\n",
    "            longest_movie = title\n",
    "            longest_duration = duration\n",
    "\n",
    "print(longest_movie, longest_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La maldición de Hill House 572\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "longest_movie = ''\n",
    "longest_duration = 0\n",
    "\n",
    "\n",
    "page = requests.get(\"https://www.imdb.com/search/title/?release_date=2018&sort=num_votes,desc&page=1&ref_=adv_nxt\")\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "head_title = soup.find_all(\"h3\",class_=\"lister-item-header\")\n",
    "head_duration = soup.find_all(\"span\",class_=\"runtime\")\n",
    "for j, k in zip(head_title, head_duration):\n",
    "    title = j.a.string\n",
    "    duration = ''.join(filter(str.isdigit, k.string))\n",
    "    duration = int(duration)\n",
    "    \n",
    "    if (duration > longest_duration):\n",
    "        longest_movie = title\n",
    "        longest_duration = duration\n",
    "\n",
    "\n",
    "print(longest_movie, longest_duration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image with maximum area\n",
    "\n",
    "From this website :\n",
    "Find and print the src of the <img> tag which occupies the maximum area on the page.\n",
    "Note :\n",
    "Ignore images which doesn't have height or width attributes\n",
    "Output Format :\n",
    "src_of_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "\n",
    "page = requests.get(url)\n",
    "data = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "images = data.find_all(\"img\", {\"height\": True, \"width\": True})\n",
    "\n",
    "max_area = 0\n",
    "src_output = ''\n",
    "for i in images:\n",
    "    width = int(i[\"width\"])\n",
    "    height = int(i[\"height\"])\n",
    "    area = width * height\n",
    "    if area > max_area:\n",
    "        max_area = area\n",
    "        src_output = i['src']\n",
    "        \n",
    "print(src_output)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quotes with tag humor\n",
    "\n",
    "Find all the quotes that have the tag as \"humor\" from this website\n",
    "Output Format :\n",
    "quote1\n",
    "quote2\n",
    "quote3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "“A day without sunshine is like, you know, night.”\n",
      "“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”\n",
      "“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”\n",
      "“All you need is love. But a little chocolate now and then doesn't hurt.”\n",
      "“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\n",
      "“Some people never go crazy. What truly horrible lives they must lead.”\n",
      "“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”\n",
      "“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”\n",
      "“The reason I talk to myself is because I’m the only one whose answers I accept.”\n",
      "“I am free of all prejudice. I hate everyone equally. ”\n",
      "“A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "data = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "quotes_list = []\n",
    "\n",
    "for p in range(1,11):\n",
    "    response = requests.get(url + '/page/' + str(p))\n",
    "    data = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    quotes = data.find_all('div', {'class':'quote'})\n",
    "    for i in quotes:\n",
    "        tags = i.find_all('a', {'class':'tag'})\n",
    "        for j in tags:\n",
    "            if (j.string == 'humor'):\n",
    "                quote = i.find('span', {'class':'text'}).string\n",
    "                quotes_list.append(quote)\n",
    "        \n",
    "for i in quotes_list:\n",
    "    print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print all authors\n",
    "\n",
    "Find and print the names of all the different authors from all pages of this website\n",
    "Note : Print the names of all authors line wise sorted in dictionary order\n",
    "Output Format :\n",
    "author1\n",
    "author2\n",
    "author3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein\n",
      "Alexandre Dumas fils\n",
      "Alfred Tennyson\n",
      "Allen Saunders\n",
      "André Gide\n",
      "Ayn Rand\n",
      "Bob Marley\n",
      "C.S. Lewis\n",
      "Charles Bukowski\n",
      "Charles M. Schulz\n",
      "Douglas Adams\n",
      "Dr. Seuss\n",
      "E.E. Cummings\n",
      "Eleanor Roosevelt\n",
      "Elie Wiesel\n",
      "Ernest Hemingway\n",
      "Friedrich Nietzsche\n",
      "Garrison Keillor\n",
      "George Bernard Shaw\n",
      "George Carlin\n",
      "George Eliot\n",
      "George R.R. Martin\n",
      "Harper Lee\n",
      "Haruki Murakami\n",
      "Helen Keller\n",
      "J.D. Salinger\n",
      "J.K. Rowling\n",
      "J.M. Barrie\n",
      "J.R.R. Tolkien\n",
      "James Baldwin\n",
      "Jane Austen\n",
      "Jim Henson\n",
      "Jimi Hendrix\n",
      "John Lennon\n",
      "Jorge Luis Borges\n",
      "Khaled Hosseini\n",
      "Madeleine L'Engle\n",
      "Marilyn Monroe\n",
      "Mark Twain\n",
      "Martin Luther King Jr.\n",
      "Mother Teresa\n",
      "Pablo Neruda\n",
      "Ralph Waldo Emerson\n",
      "Stephenie Meyer\n",
      "Steve Martin\n",
      "Suzanne Collins\n",
      "Terry Pratchett\n",
      "Thomas A. Edison\n",
      "W.C. Fields\n",
      "William Nicholson\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "data = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "authors_list = []\n",
    "\n",
    "\n",
    "for p in range(1,11):\n",
    "    response = requests.get(url + '/page/'+str(p))\n",
    "    data = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    authors = data.find_all('small', {'class':'author'})\n",
    "    for i in authors:\n",
    "        author = i.string\n",
    "        if author not in authors_list:\n",
    "            authors_list.append(i.string)\n",
    "\n",
    "authors_list.sort()\n",
    "for i in authors_list:\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth Date of authors\n",
    "\n",
    "Find the birth date of authors whose name start with 'J' from this website\n",
    "Note : Print a dictionary containing the name as key and the birth date as value.The Names of authors should be alphabetically sorted.\n",
    "Output Format :\n",
    "{'author_1': 'month day, year', 'author_2': 'month day, year', ....}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'J.K. Rowling': 'July 31, 1965', 'Jane Austen': 'December 16, 1775', 'Jim Henson': 'September 24, 1936', 'Jorge Luis Borges': 'August 24, 1899', 'James Baldwin': 'August 02, 1924', 'J.R.R. Tolkien': 'January 03, 1892', 'J.D. Salinger': 'January 01, 1919', 'John Lennon': 'October 09, 1940', 'Jimi Hendrix': 'November 27, 1942', 'J.M. Barrie': 'May 09, 1860'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "data = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "authors_dict = dict()\n",
    "\n",
    "\n",
    "for p in range(1,11):\n",
    "    response = requests.get(url + '/page/'+str(p))\n",
    "    data = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    quote_block = data.find_all('div', {'class':'quote'})\n",
    "    for i in quote_block:\n",
    "        children = i.findChildren()\n",
    "        details = children[1].findChildren()\n",
    "        author = details[0].string\n",
    "        if (author[0] != 'J'):\n",
    "            continue\n",
    "        url_author = url + details[1]['href']\n",
    "        response2 = requests.get(url_author)\n",
    "        data2 = BeautifulSoup(response2.content,'html.parser')\n",
    "        birthday = data2.find('span',{'class':'author-born-date'})\n",
    "        birthday = birthday.string\n",
    "        authors_dict[author] = birthday\n",
    "    \n",
    "sorted_dict = dict(sorted(authors_dict.items(), key=lambda x: x[0]))\n",
    "print(sorted_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quotes by Albert Einstein\n",
    "\n",
    "Find all the quotes by Albert Einstein(in the order they appear on the page) from this website\n",
    "Note : Fetch data from all the pages.\n",
    "\n",
    "Output Format :\n",
    "quote1\n",
    "quote2\n",
    "quote3\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "“If you can't explain it to a six year old, you don't understand it yourself.”\n",
      "“If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”\n",
      "“Logic will get you from A to Z; imagination will get you everywhere.”\n",
      "“Any fool can know. The point is to understand.”\n",
      "“Life is like riding a bicycle. To keep your balance, you must keep moving.”\n",
      "“If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music.”\n",
      "“Anyone who has never made a mistake has never tried anything new.”\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://quotes.toscrape.com'\n",
    "response = requests.get(url)\n",
    "data = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "\n",
    "for p in range(1,11):\n",
    "    response = requests.get(url + '/page/' + str(p))\n",
    "    data = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    quote_block = data.find_all('div', {'class':'quote'})\n",
    "    for i in quote_block:\n",
    "        children = i.findChildren()\n",
    "        author = children[1].findChildren()[0].string\n",
    "        if author != 'Albert Einstein':\n",
    "            continue\n",
    "        quote = children[0].string\n",
    "        print(quote)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Categories\n",
    "\n",
    "Print the name of all categories which are present this website.\n",
    "Output Format :\n",
    " Category1 Name\n",
    " Category2 Name\n",
    " Category3 Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel\n",
      "Mystery\n",
      "Historical Fiction\n",
      "Sequential Art\n",
      "Classics\n",
      "Philosophy\n",
      "Romance\n",
      "Womens Fiction\n",
      "Fiction\n",
      "Childrens\n",
      "Religion\n",
      "Nonfiction\n",
      "Music\n",
      "Default\n",
      "Science Fiction\n",
      "Sports and Games\n",
      "Add a comment\n",
      "Fantasy\n",
      "New Adult\n",
      "Young Adult\n",
      "Science\n",
      "Poetry\n",
      "Paranormal\n",
      "Art\n",
      "Psychology\n",
      "Autobiography\n",
      "Parenting\n",
      "Adult Fiction\n",
      "Humor\n",
      "Horror\n",
      "History\n",
      "Food and Drink\n",
      "Christian Fiction\n",
      "Business\n",
      "Biography\n",
      "Thriller\n",
      "Contemporary\n",
      "Spirituality\n",
      "Academic\n",
      "Self Help\n",
      "Historical\n",
      "Christian\n",
      "Suspense\n",
      "Short Stories\n",
      "Novels\n",
      "Health\n",
      "Politics\n",
      "Cultural\n",
      "Erotica\n",
      "Crime\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('http://books.toscrape.com/')\n",
    "\n",
    "html_text = response.text\n",
    "data = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "categories_html = data.find(class_='nav nav-list')\n",
    "categories_names = categories_html.find_all('a')\n",
    "for i in categories_names[1:]:\n",
    "    print(i.text.strip())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First 5 Blogs\n",
    "|\n",
    "Print the title of the first 5 blogs written by Coding Ninjas\n",
    "Note : Print the blog names line wise\n",
    "Output Format :\n",
    "blog_name_1\n",
    "blog_name_2\n",
    "blog_name_3\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Development Trends to watch out for in 2020\n",
      "Web Development: Interviews and You!\n",
      "Get equipped for the Technical Interviews\n",
      "Explore more about the projects in Web Development\n",
      "5G to be a major gamechanger for Edu-tech platforms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://medium.com/codingninjas-blog'\n",
    "response = requests.get(url)\n",
    "data = response.content\n",
    "data = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "blog_titles = data.find_all('div', {'class':'section-content'})\n",
    "for i in blog_titles[:5]:\n",
    "    print(i.h3.text.strip())\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WebDriverWait' from 'selenium.webdriver' (c:\\Users\\Ansia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ansia\\Code\\DataScienceCourse\\19_WebScraping_BeautifulSoup\\Assignment.ipynb Cell 26\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ansia/Code/DataScienceCourse/19_WebScraping_BeautifulSoup/Assignment.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebdriver\u001b[39;00m \u001b[39mimport\u001b[39;00m WebDriverWait\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'WebDriverWait' from 'selenium.webdriver' (c:\\Users\\Ansia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import WebDriverWait\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cb3211df066898bfa33f604a9f08045f479368c210eaa95a0a200394b2af790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
