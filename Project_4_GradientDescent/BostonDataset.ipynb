{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent - Boston Dataset\n",
    "1. Code Gradient Descent for N features and come with predictions.\n",
    "2. Try and test with various combinations of learning rates and number of iterations.\n",
    "3. Try using Feature Scaling, and see if it helps you in getting better results. \n",
    "   \n",
    "Read Instructions carefully\n",
    "1. Use Gradient Descent as a training algorithm and submit results predicted.\n",
    "2. Files are in csv format, you can use genfromtxt function in numpy to load data from csv file. Similarly you can use savetxt function to save data into a file.\n",
    "3. Submit a csv file with only predictions for X test data. File name should not have spaces. File should not have any headers and should only have one column i.e. predictions. Also predictions shouldn't be in exponential form. \n",
    "4. Your score is based on coefficient of determination.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt(\"DataSets\\\\0000000000002417_training_boston_x_y_train.csv\", delimiter = ',', skip_header=0)\n",
    "print(data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(points, learning_rate, m):\n",
    "    m_slope = np.zeros(14)\n",
    "    N = len(points)\n",
    "    for i in range(N):\n",
    "        x = points[i, 0:13]\n",
    "        x = np.append(x, 1)\n",
    "        y = points[i, 13]\n",
    "        # print(f\"x = {x} \\n y = {y} \\n N = {N} \\n m_slope = {m_slope} \\n m = {m}\")\n",
    "        for j in range(14):\n",
    "            m_slope[j] += (-2/N) * (y - (m * x).sum()) * x[j]\n",
    "        m = m - (learning_rate * m_slope)\n",
    "    return m\n",
    "\n",
    "def gradient_n(points, learning_rate, num_iterations):\n",
    "    m = np.zeros(14)\n",
    "    _cost = 0\n",
    "    for i in range(num_iterations):\n",
    "        m = step_gradient(points, learning_rate, m)\n",
    "        _cost = cost(points, m)\n",
    "        # print(i, \" Cost:\\t\", _cost)\n",
    "    print(f\"Cost after {num_iterations} iterations: {_cost}\")\n",
    "    return m\n",
    "\n",
    "def cost(points, m):\n",
    "    total_cost = 0\n",
    "    N = len(points)\n",
    "    for i in range(N):\n",
    "        x = points[i, 0:13]\n",
    "        x = np.append(x, 1)\n",
    "        y = points[i, 13]\n",
    "        total_cost += (1/N)*((np.sum(m * x) - y)**2)\n",
    "    return total_cost\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and test with various combinations of learning rates and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 100 iterations: 25.541343677214183\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_iterations = 100\n",
    "m = gradient_n(data, learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 100 iterations: 23.637134760598574\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_iterations = 100\n",
    "m = gradient_n(data, learning_rate, num_iterations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less cost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 500 iterations: 23.610743067473866\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_iterations = 500\n",
    "m = gradient_n(data, learning_rate, num_iterations)\n",
    "\n",
    "c = m[13]\n",
    "m = m[:13]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output mean:  22.88352573240686\n"
     ]
    }
   ],
   "source": [
    "data_test = np.genfromtxt(\"DataSets\\\\0000000000002417_test_boston_x_test.csv\", delimiter = ',')\n",
    "n = data_test.shape[0]\n",
    "y_t = []\n",
    "\n",
    "for i in range(n):\n",
    "    x = data_test[i,:]\n",
    "    y_t.append(np.sum(x*m) + c)\n",
    "\n",
    "print(\"Predicted output mean: \", np.mean(y_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.66871235226221\n",
      "28.643133489286292\n",
      "22.488403634987225\n",
      "24.568838337195412\n",
      "21.19403839152967\n",
      "3.064350505227157\n",
      "30.46937291079713\n",
      "25.118961211909106\n",
      "18.986084743194603\n",
      "23.830629807496265\n",
      "24.46267855717056\n",
      "18.024171351153996\n",
      "18.011335209713025\n",
      "21.676454642272184\n",
      "41.16756597798843\n",
      "24.040844113320865\n",
      "24.673826110475737\n",
      "27.817232057933992\n",
      "20.50529533394238\n",
      "31.25989270185057\n",
      "23.954141654629407\n",
      "24.154214416921977\n",
      "33.00636989032705\n",
      "35.88366881130343\n",
      "32.19511807291552\n",
      "16.26093676383558\n",
      "23.48878604671772\n",
      "32.9990748419126\n",
      "24.36177270730707\n",
      "33.20580330683998\n",
      "17.296738659847133\n",
      "26.200242060480917\n",
      "23.545957192261376\n",
      "25.398034200835248\n",
      "14.85121715165456\n",
      "29.568914108616\n",
      "26.467527796425667\n",
      "20.717177640613595\n",
      "24.26569868594361\n",
      "9.535800517712133\n",
      "7.831006888995004\n",
      "29.07323116091462\n",
      "29.564986677081837\n",
      "19.583989951313207\n",
      "20.276718501294553\n",
      "3.061145459175041\n",
      "39.559328100661276\n",
      "25.648606081784944\n",
      "30.514683298549578\n",
      "16.855652127884323\n",
      "17.609230833410255\n",
      "40.71810396218609\n",
      "17.59493501232943\n",
      "21.275266642562194\n",
      "15.96810747474056\n",
      "21.824541889047513\n",
      "18.63247873291378\n",
      "23.596117723244166\n",
      "14.053154783373873\n",
      "17.15112558033013\n",
      "14.434290494721004\n",
      "28.949843572853574\n",
      "25.444928537240884\n",
      "25.757587793448046\n",
      "17.024042911771126\n",
      "17.122065076784377\n",
      "34.69135279435137\n",
      "17.02636418751329\n",
      "26.413355256196027\n",
      "22.39959034716582\n",
      "29.517575049720868\n",
      "26.287854975299464\n",
      "17.551201035124322\n",
      "5.5066542114965\n",
      "36.58365589547118\n",
      "25.40324364752919\n",
      "30.10166330832525\n",
      "27.41144111261964\n",
      "16.054257745987776\n",
      "32.59825492673379\n",
      "19.320331117117338\n",
      "22.930495607371114\n",
      "22.911086644073194\n",
      "8.904693765066966\n",
      "17.33431450867505\n",
      "29.528901335879652\n",
      "27.281760635567835\n",
      "5.912199960774053\n",
      "20.953450280399498\n",
      "19.924364079988386\n",
      "22.32842941511033\n",
      "20.84698232214527\n",
      "21.012257229873818\n",
      "13.52521451802189\n",
      "20.057785497600147\n",
      "25.641359958093307\n",
      "40.204089126714535\n",
      "18.87084793187259\n",
      "33.826595529598364\n",
      "27.050806825789262\n",
      "28.640018819398882\n",
      "21.938009572995863\n",
      "25.52354353626533\n",
      "31.57774283064318\n",
      "17.180450539632826\n",
      "26.50299147398993\n",
      "21.742190774933807\n",
      "36.91796260490571\n",
      "22.333654615250254\n",
      "16.45341778899354\n",
      "27.720138675645767\n",
      "-0.02827240055452407\n",
      "14.384569654721734\n",
      "15.986327164282773\n",
      "36.09706838558449\n",
      "21.121650199093054\n",
      "21.148923013358072\n",
      "25.486895948579868\n",
      "22.146697788035922\n",
      "18.53019588057697\n",
      "13.939747109376654\n",
      "35.46423625186386\n",
      "23.057738962500977\n",
      "24.57334672794192\n",
      "18.090379260736977\n",
      "21.10544352647961\n",
      "14.506083291752153\n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"DataSets\\\\predictions_boston.csv\", y_t, delimiter =',', fmt='%.10f')\n",
    "\n",
    "for i in y_t:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
